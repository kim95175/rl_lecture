{"cells":[{"cell_type":"markdown","metadata":{"id":"mKWoCMdcywYi"},"source":["## Prerequisites \n","먼저 학습에 사용될 Library를 설치합니다."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zVD5yS3Mysfe"},"outputs":[],"source":["!apt install xvfb\n","!pip install pyvirtualdisplay\n","!pip install numpy\n","!pip install gym==0.22.* pygame"]},{"cell_type":"markdown","metadata":{"id":"1ouGdkr2zYeI"},"source":["## Improt the dependencies 📚\n","\n","Q Learning 실습에 사용할 Library를 불러옵니다.\n","- `Numpy` Q table을 만들고 저장합니다.\n","- `OpenAI Gym` FrozenLake 환경을 불러옵니다.\n","- `Random` 랜덤한 수를 생성합니다.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Wd3dwX-r0EzL"},"outputs":[],"source":["import numpy as np\n","import gym\n","import random\n","\n","from matplotlib import pyplot as plt\n","%matplotlib inline\n","from time import sleep\n","from IPython import display"]},{"cell_type":"markdown","metadata":{"id":"hyr2hpVVRivQ"},"source":["# OpenAI Gym\n","OpenAI에서는 다양한 강화학습 알고리즘 코드나 환경들을 제공합니다.\n","\n","OpenAI Gym은 강화학습 환경의 표준적인 Baseline을 제공합니다. 또한 여러가지 sample 환경들을 제공합니다.\n","\n","이외에도 다양한 환경들이 gym style에 맟춰서 개발되므로 이에 맟춰서 코드를 작성해 하나의 알고리즘으로 다양한 환경에서 실험할 수 있습니다.\n","\n","https://gym.openai.com/"]},{"cell_type":"markdown","metadata":{"id":"i5u2UOzKGvPG"},"source":["# FrozenLake 🧊\n","FrozenLake 환경은 grid world에서 시작점에서 시작해 목표 지점까지 도착하는 것이 목표인 환경입니다. \n","\n","### State\n","FrozenLake는 4x4 환경에서 진행됩니다. Agent가 존재 할 수 있는 16가지 위치(0~15)\n","가 state가 됩니다.\n","\n","### Action\n","FrozenLake의 Action은 상하좌우로 움직일 수 있습니다.\n","- 0: LEFT\n","- 1: DOWN\n","- 2: RIGHT\n","- 3: UP\n","\n","### Reward\n","목적지에 도달하면 1점을, 이외에는 0점을 얻습니다."]},{"cell_type":"markdown","metadata":{"id":"8XoUnqI7UtjX"},"source":["이번 실습은 Gym FrozenLake 환경에서 진행합니다. \n","\n","본격적으로 시작하기 전에 FrozenLake-v0 환경에서 랜덤한 액션을 하는 에이전트를 실행시켜 봅시다."]},{"cell_type":"markdown","metadata":{"id":"eacrD6gqJZHW"},"source":["\n","먼저 환경 오브젝트를 생성합니다. gym.make(환경이름)을 사용합니다.\n","\n","env.observation_space는 환경의 state와 관련된 정보를 가진 오브젝트입니다.\n","\n","env.action_space는 환경의 action과 관련된 정보를 가진 오브젝트입니다. "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"i6L0zi1jYIJr"},"outputs":[],"source":["env = gym.make(\"FrozenLake-v1\", is_slippery=False)\n","env.reset()\n","\n","print(f\"obsrvation_space = {env.observation_space}, {env.observation_space.n}\")\n","\n","print(f\"action_space = {env.action_space}, {env.action_space.n}\")\n","\n","print(f\"env.reset() = {env.reset()}\")\n","\n","plt.imshow(env.render(mode='rgb_array'))    "]},{"cell_type":"markdown","metadata":{"id":"gFraYDmHYbwo"},"source":["`env.reset` 환경을 초기화하고 첫 state를 반환합니다.\n","\n","`env.render` 환경을 visualize 해주는 함수입니다.\n","\n","`env.action_space.sample` action_space의 랜덤한 action을 반환합니다. ( 여기서는 0~3사이의 랜덤한 정수 )\n"]},{"cell_type":"code","source":["obs, reward, done, info = env.step(0)\n","plt.imshow(env.render(mode='rgb_array')) \n","\n","print(f'obs: {obs}')\n","print(f'reward: {reward}')\n","print(f'done: {done}')\n","print(f'info: {info}')"],"metadata":{"id":"ZpmldiKFHPhg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MDNBtEPIJgH7"},"source":["`env.step(action)` 해당 action을 환경에서 실행하고, obs, reward, done, info값을 받아옵니다.\n","\n","`obs(Observation)`는 다음 state를 나타냅니다. (State t+1)\n","\n","`Reward`는 action을 통해 얻은 보상입니다.\n","\n","`Done`은 현재 환경이 끝났는지 여부를 알려줍니다.\n","\n","`Info`는 부가적인 정보를 담은 dict입니다. 대부분 학습에 사용되지 않습니다."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"S4H-b6zhXRPA"},"outputs":[],"source":["env = gym.make(\"FrozenLake-v1\", is_slippery=False)\n","\n","def random_episode(env):\n","\n","    #env.reset() -> reset environment and returns the initial state\n","    state = env.reset()\n","    done = False\n","    step_count = 0\n","    while not done:\n","        #env.action_space.sample() -> returns random value within the action space\n","        action = env.action_space.sample()\n","\n","        #env.step(action) -> do action in environment, returns next state, reward, done, and extra values\n","        observation, reward, done, info = env.step(action)\n","        state = observation\n","\n","        display.clear_output(wait=True)\n","        print(f\"========= step {step_count} ==========\")\n","        print(f'action: {action}')\n","        print(f'obs: {observation}')\n","        print(f'reward: {reward}')\n","        print(f'done: {done}')\n","        print(f'info: {info}')\n","  \n","        plt.imshow(env.render(mode='rgb_array')) \n","        display.display(plt.gcf())\n","        \n","        \n","        sleep(0.3)\n","        step_count += 1\n","        if step_count > 100:\n","            break\n","\n","random_episode(env)"]},{"cell_type":"markdown","metadata":{"id":"TCwEFNKntPgJ"},"source":["# Q Learning\n","다음으로는 모델을 알지 못해도 학습할 수 있는 model-free 방식인 q learning으로 학습하는 에이전트를 구현해 보겠습니다.\n"]},{"cell_type":"markdown","metadata":{"id":"fcJWDUNHHUP3"},"source":["Q Table에는 해당 (state, action) 쌍에서의 quality를 뜻하는 값을 저장합니다.\n","\n","이때 초기에는 1의 값을 갖고 다음과 같은 식에 의해 점차 update 됩니다.\n","\n","$$ Q({\\small state}, {\\small action}) \\leftarrow (1 - \\alpha) Q({\\small state}, {\\small action}) + \\alpha \\Big({\\small reward} + \\gamma \\max_{a} Q({\\small next \\ state}, {\\small all \\ actions})\\Big) $$"]},{"cell_type":"markdown","metadata":{"id":"-1dI5VPki57-"},"source":["Q Learning에서는 Agent의 Exploration을 위해 `epsilon-greedy policy`를 사용합니다.\n","\n","epsilon의 확률로 랜덤한 행동을 하고, 아닐경우 q_table[state] 에서 가장 기댓값이 높은 액션을 하게 됩니다."]},{"cell_type":"markdown","metadata":{"id":"bZ44RAJgi73k"},"source":["스켈레톤 코드에서 Q Learning을 구현해 봅시다"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9z2-l7epxkTC"},"outputs":[],"source":["#Create environment\n","env = gym.make(\"FrozenLake-v1\", is_slippery=False)\n","\n","\n","# 0. Initialize Q table and parameters\n","\n","#Algorithm parameters: step size alpha, small epsilon\n","epsilon = 0.1 # Exploration Percentatige\n","alpha = 0.1   # Learning Rate\n","gamma = 0.99  # Discount Rate\n","\n","#Initialize Q(s, a), for all (s, a)\n","q_table = np.ones([env.observation_space.n, env.action_space.n]) #TODO\n","\n","\n","#Train function\n","def train(env, q_table, train_steps, target_score):\n","    #Values for debugging\n","    episode_num = 0\n","    reward_sum = 0\n","    total_steps = 0\n","\n","    #Loop for each episode\n","    while total_steps < train_steps: \n","        #Print average score every 100 episode\n","        episode_num += 1\n","        if episode_num % 100 == 0:\n","            print(f'{episode_num} ep : {reward_sum / 100}')\n","            if target_score <= reward_sum / 100:\n","                break\n","            reward_sum = 0\n","\n","        #Initialize S\n","        state = #TODO\n","        done = False\n","\n","        #Loop for each step of episode until S is terminal\n","        while not done:\n","            total_steps += 1\n","\n","            # 1. Select action using epsillon-greedy\n","            #Choose A from S using policy derived from Q (e.g., epsilon-greedy)\n","            if #TODO:\n","                #Best Action\n","                action = #TODO\n","            else:\n","                #Random Action\n","                action = #TODO\n","\n","            # 2. Take action and get observation\n","            #Take action A, observe S', R, done\n","            next_state, reward, done, info = #TODO\n","            reward_sum += reward\n","\n","            # 3. Update Q Table\n","            #Q(S, A) <- Q(S, A) + alpha*[R + gamma * (1-done) * max a Q(S', a) - Q(S, A)]\n","            #Q(S, A) <- (1 - alpha) Q(S, A) + alpha *[R + gamma * (1-done) * max a Q(S', a)]\n","            q_table[state][action] = #TODO\n","            \n","            #S <- S'\n","            #TODO\n","\n","    #Print total steps to learn\n","    print(f'Total steps: {total_steps}')\n","\n","#Run train\n","train(env, q_table, 100000, 0.8)"]},{"cell_type":"markdown","metadata":{"id":"_d1Fwz8MsVVw"},"source":["Q Table을 visualize 하여 어떤 식으로 학습됐는지 확인해 봅시다."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mxsABMnXr_Hx"},"outputs":[],"source":["dirs = ['◁','▽','▷','△']\n","for row in range(4):\n","    line = ''\n","    for col in range(4):\n","        #Printing the direction of maximum Q value\n","        state = col+row*4\n","        if state in (5, 7, 11, 12):\n","            line += '●'\n","        elif state == 15:\n","            line += '★'\n","        else:\n","            line += dirs[np.argmax(q_table[state])] #TODO\n","    print(line)"]},{"cell_type":"markdown","metadata":{"id":"oPIw1Qh_8kHB"},"source":["학습한 에이전트를 테스트 할 때에는 최선의 선택만을 하도록 합니다.\n","\n","env.render을 통해 잘 학습되었는지 visualize합니다.\n","\n","마지막으로 에피소드 100회 평균으로 평가해 봅니다."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"A8VwebXi8zfL"},"outputs":[],"source":["def test(env, q_table, visualize=False, step_limit=300):\n","    state = env.reset()\n","    reward_sum = 0\n","    step_count = 0\n","    done = False\n","    while not done:\n","        step_count += 1\n","        if step_count > step_limit:\n","            break\n","        \n","        #No epsilon-greedy\n","        action = np.argmax(q_table[state])\n","        state, reward, done, info = env.step(action)\n","\n","        reward_sum += reward\n","        if visualize:\n","            print(f\"========= step {step_count} ==========\")\n","            plt.imshow(env.render(mode='rgb_array'))\n","            display.display(plt.gcf())\n","            display.clear_output(wait=True)\n","            #env.render()\n","            sleep(0.2)\n","\n","    return reward_sum\n","\n","score_viz = test(env, q_table, visualize=True)\n","reward_tot = 0\n","for _ in range(100):\n","    reward_tot += test(env, q_table)\n","print(\"100 episodes average = \", reward_tot / 100)"]},{"cell_type":"markdown","metadata":{"id":"X93P9QISVsKE"},"source":["# Slippery FrozenLake ⛄\n","같은 알고리즘을 Slippery FrozenLake에서 돌려봅시다.\n","\n","`is_slippery=True`로 한 FrozenLake는 이동 방향의 좌우로 미끄러질 수 있습니다.\n","\n","예를 들어 action이 left 이동일 경우\n","- P(move left) = 1/3\n","- P(move up) = 1/3\n","- P(move down) = 1/3\n","\n","의 확률로 action이 결정되게 됩니다.\n","\n","기존 환경보다 어려운 non-deterministic한 환경입니다.\n","\n","목표 점수를 0.45로 낮추어서 학습합니다."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DY-ru8UBWZFb"},"outputs":[],"source":["#Create environment\n","env = gym.make(\"FrozenLake-v1\", is_slippery=True)\n","q_table = np.ones([env.observation_space.n, env.action_space.n])\n","train(env, q_table, 200000, 0.45)"]},{"cell_type":"code","source":["reward_tot = 0\n","for _ in range(100):\n","    reward_tot += test(env, q_table)\n","print(reward_tot / 100)"],"metadata":{"id":"NW0pDJgrUBVR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BBeQntvQa69I"},"source":["Q Table을 visualize 하여 어떤 식으로 학습됐는지 확인해 봅시다.\n","\n","주변 구멍의 반대 방향으로 이동하도록 학습된 모습을 확인할 수 있습니다."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lOunfe2KbBHD"},"outputs":[],"source":["dirs = ['◁','▽','▷','△']\n","for row in range(4):\n","    line = ''\n","    for col in range(4):\n","        #Printing the direction of maximum Q value\n","        state = col+row*4\n","        if state in (5, 7, 11, 12):\n","            line += '●'\n","        elif state == 15:\n","            line += '★'\n","        else:\n","            line += dirs[np.argmax(q_table[state])] #TODO\n","    print(line)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xZdZuBl2XgpH"},"outputs":[],"source":["#Visualize trained agent\n","score_viz = test(env, q_table, visualize=True)\n","print(score_viz)"]}],"metadata":{"colab":{"collapsed_sections":[],"name":"Q_Learning_with_FrozenLake.ipynb","provenance":[{"file_id":"1AYW1AxLRZN0R_Xg-3xuvwXNBpYiaWOzT","timestamp":1596963070163}]},"kernelspec":{"display_name":"Python 3","name":"python3"}},"nbformat":4,"nbformat_minor":0}