{"cells":[{"cell_type":"markdown","metadata":{"id":"mKWoCMdcywYi"},"source":["## Prerequisites \n","ë¨¼ì € í•™ìŠµì— ì‚¬ìš©ë  Libraryë¥¼ ì„¤ì¹˜í•©ë‹ˆë‹¤."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zVD5yS3Mysfe"},"outputs":[],"source":["!apt install xvfb\n","!pip install pyvirtualdisplay\n","!pip install numpy\n","!pip install gym==0.22.* pygame"]},{"cell_type":"markdown","metadata":{"id":"1ouGdkr2zYeI"},"source":["## Improt the dependencies ğŸ“š\n","\n","Q Learning ì‹¤ìŠµì— ì‚¬ìš©í•  Libraryë¥¼ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.\n","- `Numpy` Q tableì„ ë§Œë“¤ê³  ì €ì¥í•©ë‹ˆë‹¤.\n","- `OpenAI Gym` FrozenLake í™˜ê²½ì„ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.\n","- `Random` ëœë¤í•œ ìˆ˜ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Wd3dwX-r0EzL"},"outputs":[],"source":["import numpy as np\n","import gym\n","import random\n","\n","from matplotlib import pyplot as plt\n","%matplotlib inline\n","from time import sleep\n","from IPython import display"]},{"cell_type":"markdown","metadata":{"id":"hyr2hpVVRivQ"},"source":["# OpenAI Gym\n","OpenAIì—ì„œëŠ” ë‹¤ì–‘í•œ ê°•í™”í•™ìŠµ ì•Œê³ ë¦¬ì¦˜ ì½”ë“œë‚˜ í™˜ê²½ë“¤ì„ ì œê³µí•©ë‹ˆë‹¤.\n","\n","OpenAI Gymì€ ê°•í™”í•™ìŠµ í™˜ê²½ì˜ í‘œì¤€ì ì¸ Baselineì„ ì œê³µí•©ë‹ˆë‹¤. ë˜í•œ ì—¬ëŸ¬ê°€ì§€ sample í™˜ê²½ë“¤ì„ ì œê³µí•©ë‹ˆë‹¤.\n","\n","ì´ì™¸ì—ë„ ë‹¤ì–‘í•œ í™˜ê²½ë“¤ì´ gym styleì— ë§Ÿì¶°ì„œ ê°œë°œë˜ë¯€ë¡œ ì´ì— ë§Ÿì¶°ì„œ ì½”ë“œë¥¼ ì‘ì„±í•´ í•˜ë‚˜ì˜ ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ ë‹¤ì–‘í•œ í™˜ê²½ì—ì„œ ì‹¤í—˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n","\n","https://gym.openai.com/"]},{"cell_type":"markdown","metadata":{"id":"i5u2UOzKGvPG"},"source":["# FrozenLake ğŸ§Š\n","FrozenLake í™˜ê²½ì€ grid worldì—ì„œ ì‹œì‘ì ì—ì„œ ì‹œì‘í•´ ëª©í‘œ ì§€ì ê¹Œì§€ ë„ì°©í•˜ëŠ” ê²ƒì´ ëª©í‘œì¸ í™˜ê²½ì…ë‹ˆë‹¤. \n","\n","### State\n","FrozenLakeëŠ” 4x4 í™˜ê²½ì—ì„œ ì§„í–‰ë©ë‹ˆë‹¤. Agentê°€ ì¡´ì¬ í•  ìˆ˜ ìˆëŠ” 16ê°€ì§€ ìœ„ì¹˜(0~15)\n","ê°€ stateê°€ ë©ë‹ˆë‹¤.\n","\n","### Action\n","FrozenLakeì˜ Actionì€ ìƒí•˜ì¢Œìš°ë¡œ ì›€ì§ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n","- 0: LEFT\n","- 1: DOWN\n","- 2: RIGHT\n","- 3: UP\n","\n","### Reward\n","ëª©ì ì§€ì— ë„ë‹¬í•˜ë©´ 1ì ì„, ì´ì™¸ì—ëŠ” 0ì ì„ ì–»ìŠµë‹ˆë‹¤."]},{"cell_type":"markdown","metadata":{"id":"8XoUnqI7UtjX"},"source":["ì´ë²ˆ ì‹¤ìŠµì€ Gym FrozenLake í™˜ê²½ì—ì„œ ì§„í–‰í•©ë‹ˆë‹¤. \n","\n","ë³¸ê²©ì ìœ¼ë¡œ ì‹œì‘í•˜ê¸° ì „ì— FrozenLake-v0 í™˜ê²½ì—ì„œ ëœë¤í•œ ì•¡ì…˜ì„ í•˜ëŠ” ì—ì´ì „íŠ¸ë¥¼ ì‹¤í–‰ì‹œì¼œ ë´…ì‹œë‹¤."]},{"cell_type":"markdown","metadata":{"id":"eacrD6gqJZHW"},"source":["\n","ë¨¼ì € í™˜ê²½ ì˜¤ë¸Œì íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤. gym.make(í™˜ê²½ì´ë¦„)ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.\n","\n","env.observation_spaceëŠ” í™˜ê²½ì˜ stateì™€ ê´€ë ¨ëœ ì •ë³´ë¥¼ ê°€ì§„ ì˜¤ë¸Œì íŠ¸ì…ë‹ˆë‹¤.\n","\n","env.action_spaceëŠ” í™˜ê²½ì˜ actionê³¼ ê´€ë ¨ëœ ì •ë³´ë¥¼ ê°€ì§„ ì˜¤ë¸Œì íŠ¸ì…ë‹ˆë‹¤. "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"i6L0zi1jYIJr"},"outputs":[],"source":["env = gym.make(\"FrozenLake-v1\", is_slippery=False)\n","env.reset()\n","\n","print(f\"obsrvation_space = {env.observation_space}, {env.observation_space.n}\")\n","\n","print(f\"action_space = {env.action_space}, {env.action_space.n}\")\n","\n","print(f\"env.reset() = {env.reset()}\")\n","\n","plt.imshow(env.render(mode='rgb_array'))    "]},{"cell_type":"markdown","metadata":{"id":"gFraYDmHYbwo"},"source":["`env.reset` í™˜ê²½ì„ ì´ˆê¸°í™”í•˜ê³  ì²« stateë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.\n","\n","`env.render` í™˜ê²½ì„ visualize í•´ì£¼ëŠ” í•¨ìˆ˜ì…ë‹ˆë‹¤.\n","\n","`env.action_space.sample` action_spaceì˜ ëœë¤í•œ actionì„ ë°˜í™˜í•©ë‹ˆë‹¤. ( ì—¬ê¸°ì„œëŠ” 0~3ì‚¬ì´ì˜ ëœë¤í•œ ì •ìˆ˜ )\n"]},{"cell_type":"code","source":["obs, reward, done, info = env.step(0)\n","plt.imshow(env.render(mode='rgb_array')) \n","\n","print(f'obs: {obs}')\n","print(f'reward: {reward}')\n","print(f'done: {done}')\n","print(f'info: {info}')"],"metadata":{"id":"ZpmldiKFHPhg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MDNBtEPIJgH7"},"source":["`env.step(action)` í•´ë‹¹ actionì„ í™˜ê²½ì—ì„œ ì‹¤í–‰í•˜ê³ , obs, reward, done, infoê°’ì„ ë°›ì•„ì˜µë‹ˆë‹¤.\n","\n","`obs(Observation)`ëŠ” ë‹¤ìŒ stateë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. (State t+1)\n","\n","`Reward`ëŠ” actionì„ í†µí•´ ì–»ì€ ë³´ìƒì…ë‹ˆë‹¤.\n","\n","`Done`ì€ í˜„ì¬ í™˜ê²½ì´ ëë‚¬ëŠ”ì§€ ì—¬ë¶€ë¥¼ ì•Œë ¤ì¤ë‹ˆë‹¤.\n","\n","`Info`ëŠ” ë¶€ê°€ì ì¸ ì •ë³´ë¥¼ ë‹´ì€ dictì…ë‹ˆë‹¤. ëŒ€ë¶€ë¶„ í•™ìŠµì— ì‚¬ìš©ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"S4H-b6zhXRPA"},"outputs":[],"source":["env = gym.make(\"FrozenLake-v1\", is_slippery=False)\n","\n","def random_episode(env):\n","\n","    #env.reset() -> reset environment and returns the initial state\n","    state = env.reset()\n","    done = False\n","    step_count = 0\n","    while not done:\n","        #env.action_space.sample() -> returns random value within the action space\n","        action = env.action_space.sample()\n","\n","        #env.step(action) -> do action in environment, returns next state, reward, done, and extra values\n","        observation, reward, done, info = env.step(action)\n","        state = observation\n","\n","        display.clear_output(wait=True)\n","        print(f\"========= step {step_count} ==========\")\n","        print(f'action: {action}')\n","        print(f'obs: {observation}')\n","        print(f'reward: {reward}')\n","        print(f'done: {done}')\n","        print(f'info: {info}')\n","  \n","        plt.imshow(env.render(mode='rgb_array')) \n","        display.display(plt.gcf())\n","        \n","        \n","        sleep(0.3)\n","        step_count += 1\n","        if step_count > 100:\n","            break\n","\n","random_episode(env)"]},{"cell_type":"markdown","metadata":{"id":"TCwEFNKntPgJ"},"source":["# Q Learning\n","ë‹¤ìŒìœ¼ë¡œëŠ” ëª¨ë¸ì„ ì•Œì§€ ëª»í•´ë„ í•™ìŠµí•  ìˆ˜ ìˆëŠ” model-free ë°©ì‹ì¸ q learningìœ¼ë¡œ í•™ìŠµí•˜ëŠ” ì—ì´ì „íŠ¸ë¥¼ êµ¬í˜„í•´ ë³´ê² ìŠµë‹ˆë‹¤.\n"]},{"cell_type":"markdown","metadata":{"id":"fcJWDUNHHUP3"},"source":["Q Tableì—ëŠ” í•´ë‹¹ (state, action) ìŒì—ì„œì˜ qualityë¥¼ ëœ»í•˜ëŠ” ê°’ì„ ì €ì¥í•©ë‹ˆë‹¤.\n","\n","ì´ë•Œ ì´ˆê¸°ì—ëŠ” 1ì˜ ê°’ì„ ê°–ê³  ë‹¤ìŒê³¼ ê°™ì€ ì‹ì— ì˜í•´ ì ì°¨ update ë©ë‹ˆë‹¤.\n","\n","$$ Q({\\small state}, {\\small action}) \\leftarrow (1 - \\alpha) Q({\\small state}, {\\small action}) + \\alpha \\Big({\\small reward} + \\gamma \\max_{a} Q({\\small next \\ state}, {\\small all \\ actions})\\Big) $$"]},{"cell_type":"markdown","metadata":{"id":"-1dI5VPki57-"},"source":["Q Learningì—ì„œëŠ” Agentì˜ Explorationì„ ìœ„í•´ `epsilon-greedy policy`ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.\n","\n","epsilonì˜ í™•ë¥ ë¡œ ëœë¤í•œ í–‰ë™ì„ í•˜ê³ , ì•„ë‹ê²½ìš° q_table[state] ì—ì„œ ê°€ì¥ ê¸°ëŒ“ê°’ì´ ë†’ì€ ì•¡ì…˜ì„ í•˜ê²Œ ë©ë‹ˆë‹¤."]},{"cell_type":"markdown","metadata":{"id":"bZ44RAJgi73k"},"source":["ìŠ¤ì¼ˆë ˆí†¤ ì½”ë“œì—ì„œ Q Learningì„ êµ¬í˜„í•´ ë´…ì‹œë‹¤"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9z2-l7epxkTC"},"outputs":[],"source":["#Create environment\n","env = gym.make(\"FrozenLake-v1\", is_slippery=False)\n","\n","\n","# 0. Initialize Q table and parameters\n","\n","#Algorithm parameters: step size alpha, small epsilon\n","epsilon = 0.1 # Exploration Percentatige\n","alpha = 0.1   # Learning Rate\n","gamma = 0.99  # Discount Rate\n","\n","#Initialize Q(s, a), for all (s, a)\n","q_table = np.ones([env.observation_space.n, env.action_space.n]) #TODO\n","\n","\n","#Train function\n","def train(env, q_table, train_steps, target_score):\n","    #Values for debugging\n","    episode_num = 0\n","    reward_sum = 0\n","    total_steps = 0\n","\n","    #Loop for each episode\n","    while total_steps < train_steps: \n","        #Print average score every 100 episode\n","        episode_num += 1\n","        if episode_num % 100 == 0:\n","            print(f'{episode_num} ep : {reward_sum / 100}')\n","            if target_score <= reward_sum / 100:\n","                break\n","            reward_sum = 0\n","\n","        #Initialize S\n","        state = #TODO\n","        done = False\n","\n","        #Loop for each step of episode until S is terminal\n","        while not done:\n","            total_steps += 1\n","\n","            # 1. Select action using epsillon-greedy\n","            #Choose A from S using policy derived from Q (e.g., epsilon-greedy)\n","            if #TODO:\n","                #Best Action\n","                action = #TODO\n","            else:\n","                #Random Action\n","                action = #TODO\n","\n","            # 2. Take action and get observation\n","            #Take action A, observe S', R, done\n","            next_state, reward, done, info = #TODO\n","            reward_sum += reward\n","\n","            # 3. Update Q Table\n","            #Q(S, A) <- Q(S, A) + alpha*[R + gamma * (1-done) * max a Q(S', a) - Q(S, A)]\n","            #Q(S, A) <- (1 - alpha) Q(S, A) + alpha *[R + gamma * (1-done) * max a Q(S', a)]\n","            q_table[state][action] = #TODO\n","            \n","            #S <- S'\n","            #TODO\n","\n","    #Print total steps to learn\n","    print(f'Total steps: {total_steps}')\n","\n","#Run train\n","train(env, q_table, 100000, 0.8)"]},{"cell_type":"markdown","metadata":{"id":"_d1Fwz8MsVVw"},"source":["Q Tableì„ visualize í•˜ì—¬ ì–´ë–¤ ì‹ìœ¼ë¡œ í•™ìŠµëëŠ”ì§€ í™•ì¸í•´ ë´…ì‹œë‹¤."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mxsABMnXr_Hx"},"outputs":[],"source":["dirs = ['â—','â–½','â–·','â–³']\n","for row in range(4):\n","    line = ''\n","    for col in range(4):\n","        #Printing the direction of maximum Q value\n","        state = col+row*4\n","        if state in (5, 7, 11, 12):\n","            line += 'â—'\n","        elif state == 15:\n","            line += 'â˜…'\n","        else:\n","            line += dirs[np.argmax(q_table[state])] #TODO\n","    print(line)"]},{"cell_type":"markdown","metadata":{"id":"oPIw1Qh_8kHB"},"source":["í•™ìŠµí•œ ì—ì´ì „íŠ¸ë¥¼ í…ŒìŠ¤íŠ¸ í•  ë•Œì—ëŠ” ìµœì„ ì˜ ì„ íƒë§Œì„ í•˜ë„ë¡ í•©ë‹ˆë‹¤.\n","\n","env.renderì„ í†µí•´ ì˜ í•™ìŠµë˜ì—ˆëŠ”ì§€ visualizeí•©ë‹ˆë‹¤.\n","\n","ë§ˆì§€ë§‰ìœ¼ë¡œ ì—í”¼ì†Œë“œ 100íšŒ í‰ê· ìœ¼ë¡œ í‰ê°€í•´ ë´…ë‹ˆë‹¤."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"A8VwebXi8zfL"},"outputs":[],"source":["def test(env, q_table, visualize=False, step_limit=300):\n","    state = env.reset()\n","    reward_sum = 0\n","    step_count = 0\n","    done = False\n","    while not done:\n","        step_count += 1\n","        if step_count > step_limit:\n","            break\n","        \n","        #No epsilon-greedy\n","        action = np.argmax(q_table[state])\n","        state, reward, done, info = env.step(action)\n","\n","        reward_sum += reward\n","        if visualize:\n","            print(f\"========= step {step_count} ==========\")\n","            plt.imshow(env.render(mode='rgb_array'))\n","            display.display(plt.gcf())\n","            display.clear_output(wait=True)\n","            #env.render()\n","            sleep(0.2)\n","\n","    return reward_sum\n","\n","score_viz = test(env, q_table, visualize=True)\n","reward_tot = 0\n","for _ in range(100):\n","    reward_tot += test(env, q_table)\n","print(\"100 episodes average = \", reward_tot / 100)"]},{"cell_type":"markdown","metadata":{"id":"X93P9QISVsKE"},"source":["# Slippery FrozenLake â›„\n","ê°™ì€ ì•Œê³ ë¦¬ì¦˜ì„ Slippery FrozenLakeì—ì„œ ëŒë ¤ë´…ì‹œë‹¤.\n","\n","`is_slippery=True`ë¡œ í•œ FrozenLakeëŠ” ì´ë™ ë°©í–¥ì˜ ì¢Œìš°ë¡œ ë¯¸ë„ëŸ¬ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n","\n","ì˜ˆë¥¼ ë“¤ì–´ actionì´ left ì´ë™ì¼ ê²½ìš°\n","- P(move left) = 1/3\n","- P(move up) = 1/3\n","- P(move down) = 1/3\n","\n","ì˜ í™•ë¥ ë¡œ actionì´ ê²°ì •ë˜ê²Œ ë©ë‹ˆë‹¤.\n","\n","ê¸°ì¡´ í™˜ê²½ë³´ë‹¤ ì–´ë ¤ìš´ non-deterministicí•œ í™˜ê²½ì…ë‹ˆë‹¤.\n","\n","ëª©í‘œ ì ìˆ˜ë¥¼ 0.45ë¡œ ë‚®ì¶”ì–´ì„œ í•™ìŠµí•©ë‹ˆë‹¤."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DY-ru8UBWZFb"},"outputs":[],"source":["#Create environment\n","env = gym.make(\"FrozenLake-v1\", is_slippery=True)\n","q_table = np.ones([env.observation_space.n, env.action_space.n])\n","train(env, q_table, 200000, 0.45)"]},{"cell_type":"code","source":["reward_tot = 0\n","for _ in range(100):\n","    reward_tot += test(env, q_table)\n","print(reward_tot / 100)"],"metadata":{"id":"NW0pDJgrUBVR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BBeQntvQa69I"},"source":["Q Tableì„ visualize í•˜ì—¬ ì–´ë–¤ ì‹ìœ¼ë¡œ í•™ìŠµëëŠ”ì§€ í™•ì¸í•´ ë´…ì‹œë‹¤.\n","\n","ì£¼ë³€ êµ¬ë©ì˜ ë°˜ëŒ€ ë°©í–¥ìœ¼ë¡œ ì´ë™í•˜ë„ë¡ í•™ìŠµëœ ëª¨ìŠµì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lOunfe2KbBHD"},"outputs":[],"source":["dirs = ['â—','â–½','â–·','â–³']\n","for row in range(4):\n","    line = ''\n","    for col in range(4):\n","        #Printing the direction of maximum Q value\n","        state = col+row*4\n","        if state in (5, 7, 11, 12):\n","            line += 'â—'\n","        elif state == 15:\n","            line += 'â˜…'\n","        else:\n","            line += dirs[np.argmax(q_table[state])] #TODO\n","    print(line)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xZdZuBl2XgpH"},"outputs":[],"source":["#Visualize trained agent\n","score_viz = test(env, q_table, visualize=True)\n","print(score_viz)"]}],"metadata":{"colab":{"collapsed_sections":[],"name":"Q_Learning_with_FrozenLake.ipynb","provenance":[{"file_id":"1AYW1AxLRZN0R_Xg-3xuvwXNBpYiaWOzT","timestamp":1596963070163}]},"kernelspec":{"display_name":"Python 3","name":"python3"}},"nbformat":4,"nbformat_minor":0}